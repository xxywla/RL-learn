{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T10:12:52.075358Z",
     "start_time": "2025-12-30T10:12:52.069202Z"
    }
   },
   "cell_type": "code",
   "source": "import config",
   "id": "7bdb92628f202835",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-30T10:12:57.265249Z",
     "start_time": "2025-12-30T10:12:53.357153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = str(config.GPT2_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "id": "f116ac393ce95c59",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/rlhf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T10:13:01.038022Z",
     "start_time": "2025-12-30T10:12:59.955823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = str(config.SST2_PATH)\n",
    "dataset = load_dataset(dataset_path)\n",
    "print(dataset)\n",
    "\n",
    "ds_train, ds_val = dataset['train'], dataset['validation']\n",
    "print(ds_train[4])"
   ],
   "id": "8104d5cceec399aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "{'idx': 4, 'sentence': 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 'label': 0}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T10:13:05.084327Z",
     "start_time": "2025-12-30T10:13:04.513018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REWARD_TOKEN_ID = tokenizer.eos_token_id\n",
    "print(REWARD_TOKEN_ID)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    # 提取出文本内容\n",
    "    outputs = tokenizer(batch['sentence'])\n",
    "    # 每条数据一个评分，初始化为 0 。\n",
    "    outputs['score'] = [0] * len(outputs['input_ids'])\n",
    "    # 对每条数据的最后的reward token进行评分\n",
    "    outputs['score_index'] = [0] * len(outputs['input_ids'])\n",
    "    for i in range(len(outputs['input_ids'])):\n",
    "        # 第 i 条数据的末尾添加一个 eos token，作为reward token\n",
    "        outputs['input_ids'][i].append(REWARD_TOKEN_ID)\n",
    "        # reward token的掩码设置为 1 。\n",
    "        outputs['attention_mask'][i].append(1)\n",
    "        # 正向情感的文本评分为 1 。负向情感的评分为 0 。\n",
    "        outputs['score'][i] = float(batch['label'][i])\n",
    "        # 对 reward token 进行评分，也就是评分的索引为 reward token 的索引。\n",
    "        outputs['score_index'][i] = len(outputs['input_ids'][i]) - 1\n",
    "    return outputs\n",
    "\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"remove_columns\": ['idx', 'sentence', 'label']\n",
    "}\n",
    "\n",
    "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
    "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)\n",
    "\n",
    "print(tokenized_dataset_train[4])"
   ],
   "id": "a7c383c3cbcf3440",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:00<00:00, 127296.92 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 61496.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [261, 262, 5290, 15827, 12, 1659, 12, 1169, 12, 1008, 9310, 35478, 20954, 262, 28303, 714, 47478, 469, 510, 220, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'score': 0.0, 'score_index': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T10:13:36.851226Z",
     "start_time": "2025-12-30T10:13:36.815090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset_train.set_format(type='torch')\n",
    "tokenized_dataset_val.set_format(type='torch')\n",
    "\n",
    "print(tokenized_dataset_train[4])"
   ],
   "id": "473a18108073564d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  261,   262,  5290, 15827,    12,  1659,    12,  1169,    12,  1008,\n",
      "         9310, 35478, 20954,   262, 28303,   714, 47478,   469,   510,   220,\n",
      "        50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'score': tensor(0.), 'score_index': tensor(20)}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T10:13:51.534434Z",
     "start_time": "2025-12-30T10:13:50.814744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset_train = tokenized_dataset_train.filter(lambda x: len(x['input_ids']) > 6)\n",
    "tokenized_dataset_val = tokenized_dataset_val.filter(lambda x: len(x['input_ids']) > 6)\n",
    "\n",
    "print(len(tokenized_dataset_train))"
   ],
   "id": "8faae07adb373427",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 67349/67349 [00:00<00:00, 96878.86 examples/s] \n",
      "Filter: 100%|██████████| 872/872 [00:00<00:00, 88046.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T11:49:37.732032Z",
     "start_time": "2025-12-30T11:49:37.087279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import GPT2RewardHead\n",
    "\n",
    "model = GPT2RewardHead(model_path)"
   ],
   "id": "f8f33ab98f1da365",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T11:50:02.181094Z",
     "start_time": "2025-12-30T11:50:02.058547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# 还是将 eos token 作为 pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    'collate_fn': data_collator\n",
    "}\n",
    "train_dataloader = DataLoader(tokenized_dataset_train, **dataloader_params)\n",
    "val_dataloader = DataLoader(tokenized_dataset_val, **dataloader_params)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "\n",
    "print(batch['input_ids'][1])\n",
    "print(batch['attention_mask'][1])\n",
    "print(batch['score'][1])\n",
    "print(batch['score_index'][1])\n",
    "print(tokenizer.decode(batch['input_ids'][1]))\n",
    "print(batch['attention_mask'][1].nonzero()[-1])"
   ],
   "id": "2d7d78f0f15cb42f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[ 1169,  2818,  1986,  ..., 50256, 50256, 50256],\n",
      "        [10594,   307,   308,  ..., 50256, 50256, 50256],\n",
      "        [11246,  5141,   508,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [13959,  1014,   832,  ..., 50256, 50256, 50256],\n",
      "        [   11,   788,   484,  ..., 50256, 50256, 50256],\n",
      "        [ 8988,   572,   355,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'score': tensor([1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.]), 'score_index': tensor([14, 20, 16,  9, 11,  6,  6, 30, 56, 47, 12, 28, 14,  6,  8,  6,  8,  6,\n",
      "        33,  9, 12,  6, 12, 11, 22,  9,  7,  6, 21, 36, 22, 18])})\n",
      "tensor([10594,   307,   308, 10316,   284,  2687,   407,  2747,  8802,  1335,\n",
      "          284,   262,  3807,   705,    82, 22066,   290, 14897, 14733,   220,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(0.)\n",
      "tensor(20)\n",
      "will be greek to anyone not predisposed to the movie 's rude and crude humor <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "tensor([20])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T11:50:21.955476Z",
     "start_time": "2025-12-30T11:50:19.736902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "print(outputs.shape)"
   ],
   "id": "98bb4f575e4adffd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 57])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T11:50:49.726637Z",
     "start_time": "2025-12-30T11:50:49.711869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# 二分类交叉熵损失\n",
    "criterion = nn.BCELoss()\n",
    "num_epochs = 1  # N+ Implementation Detail paper"
   ],
   "id": "45eda62ca0eaa466",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T11:51:12.904115Z",
     "start_time": "2025-12-30T11:51:12.899522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        inputs = batch.to(device)\n",
    "        model_inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask']\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            # 对输出进行评分\n",
    "            scores = model(**model_inputs)\n",
    "            # 批次中每条数据的索引\n",
    "            batch_indices = torch.arange(scores.shape[0])\n",
    "            # 根据索引拿出评分，也就是reward token的评分\n",
    "            score = scores[batch_indices, inputs['score_index']]\n",
    "            # 目标评分，0 或者 1 。\n",
    "            target = inputs['score']\n",
    "            # 计算误差\n",
    "            loss = criterion(score, target)\n",
    "        total_loss += loss.item()\n",
    "    print('validation loss:', total_loss / len(val_dataloader))"
   ],
   "id": "7a369a29c1db1edb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:31:39.511096Z",
     "start_time": "2025-12-30T11:51:50.022486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "\n",
    "validate()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs = batch.to(device)\n",
    "        model_inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask']\n",
    "        }\n",
    "        scores = model(**model_inputs)\n",
    "        batch_indices = torch.arange(scores.shape[0])\n",
    "        score = scores[batch_indices, inputs['score_index']]\n",
    "        target = inputs['score']\n",
    "        loss = criterion(score, target)\n",
    "        # 三部曲：清空梯度 ⟶ 反向传播计算梯度 ⟶ 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(loss.item())\n",
    "    validate()"
   ],
   "id": "f8e59142ea175f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 4.739590099879673\n",
      "3.476541519165039\n",
      "0.35558801889419556\n",
      "validation loss: 0.24328557381938612\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:32:13.526249Z",
     "start_time": "2025-12-30T12:32:12.542385Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), str(config.REWARD_MODEL_PATH))",
   "id": "a10ef9530872978b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:33:25.576275Z",
     "start_time": "2025-12-30T12:33:11.410642Z"
    }
   },
   "cell_type": "code",
   "source": "validate()",
   "id": "a6676accdb160eda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.24330428748258523\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:33:46.066920Z",
     "start_time": "2025-12-30T12:33:30.671709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "    inputs = batch.to(device)\n",
    "    model_inputs = {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask']\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        scores = model(**model_inputs)\n",
    "        batch_indices = torch.arange(scores.shape[0])\n",
    "        score = scores[batch_indices, inputs['score_index']]\n",
    "        target = inputs['score']\n",
    "    predictions = (score > 0.5).int()\n",
    "\n",
    "    all_predictions.extend(predictions.cpu().numpy())\n",
    "    all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "confusion_matrix(all_labels, all_predictions)"
   ],
   "id": "2ea70e8659875f46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[384,  40],\n",
       "       [ 41, 402]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
