{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import config",
   "id": "44f444ea028b5783",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = str(config.SST2_PATH)\n",
    "ds = load_dataset(dataset_path)\n",
    "ds_train, ds_val = ds['train'], ds['validation']\n",
    "\n",
    "print(ds)\n",
    "print(ds_train)\n",
    "print(ds_train[6])\n",
    "print(ds_train[:10])"
   ],
   "id": "c70722cb66eb16fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = config.GPT2_PATH\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ],
   "id": "7ef7dded8258f151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 只使用文本内容sentence，不使用情感标签\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'])\n",
    "\n",
    "\n",
    "map_kwargs = {\n",
    "    'batched': True,\n",
    "    'batch_size': 512,\n",
    "    'remove_columns': ['idx', 'sentence', 'label']\n",
    "}\n",
    "\n",
    "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
    "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)\n",
    "\n",
    "print(tokenized_dataset_train[0])\n",
    "print(tokenized_dataset_train[5:10])"
   ],
   "id": "8c2af80d1aa9f315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, seq in enumerate(tokenized_dataset_train[5:10]['input_ids']):\n",
    "    print(f'{i + 1}: {tokenizer.decode(seq)}')"
   ],
   "id": "cc86e355b429d0a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(tokenized_dataset_train), len(tokenized_dataset_val))\n",
    "\n",
    "tokenized_dataset_train = tokenized_dataset_train.filter(lambda x: len(x['input_ids']) > 5)\n",
    "tokenized_dataset_val = tokenized_dataset_val.filter(lambda x: len(x['input_ids']) > 5)\n",
    "\n",
    "print(len(tokenized_dataset_train), len(tokenized_dataset_val))"
   ],
   "id": "a427654b8577f001",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenized_dataset_train.set_format(type='torch')\n",
    "tokenized_dataset_val.set_format(type='torch')\n",
    "\n",
    "print(tokenized_dataset_train[0])\n",
    "print(tokenized_dataset_train[:5])"
   ],
   "id": "955172a85a85bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 检查pad token的设置（应该为空）\n",
    "print(tokenizer.pad_token)\n",
    "# # 检查eos token的设置\n",
    "print(tokenizer.eos_token)\n",
    "# N+ Implementation论文（第5页）说法不同\n",
    "# 但我们会使用attention_mask来移除用于填充的额外eos_token\n",
    "# 通过attention_mask来区分真正的结束token和用于填充的token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "2187b83ed21e23c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# mlm=False，将数据整理成“因果语言建模”需要的数据格式\n",
    "# “因果语言建模”就是“预测下一个token”类型的任务，也就是gpt风格的自回归模型\n",
    "# 如果mlm=True，那么数据整理成bert风格的任务所需的数据格式\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)  # labels\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 16,  # 6G显存正好够用\n",
    "    'collate_fn': data_collator\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset_train, **dataloader_params)\n",
    "val_dataloader = DataLoader(tokenized_dataset_val, **dataloader_params)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['input_ids'][0])\n",
    "print(batch['labels'][0])\n",
    "print(batch['attention_mask'][0])"
   ],
   "id": "317febf56299e553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 要更新的是model的参数\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# 一般sft会训练1个epoch\n",
    "num_epochs = 1"
   ],
   "id": "1928cf6cc9b58209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss  # 损失\n",
    "            total_loss += loss.item()\n",
    "    print(f'val_loss at {epoch} epoch:', total_loss / len(val_dataloader))"
   ],
   "id": "da6e4736ad2fc0ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "validate(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    validate(epoch + 1)"
   ],
   "id": "bda5991a14e92494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sft_model_path = str(config.GPT2_SFT_PATH)\n",
    "model.save_pretrained(sft_model_path)\n",
    "tokenizer.save_pretrained(sft_model_path)"
   ],
   "id": "faff06beb0ab693a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from pprint import pprint\n",
    "\n",
    "g = pipeline('text-generation', model=sft_model_path)\n",
    "set_seed(42)\n",
    "pprint(g(\"this is a\", max_length=30, num_return_sequences=1))"
   ],
   "id": "c103af49aa5d1596",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
