{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:34:02.998297Z",
     "start_time": "2025-12-30T12:33:59.507685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import config\n",
    "from model import GPT2RewardModel\n",
    "import torch"
   ],
   "id": "ea38dd52aae1d9df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/rlhf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-30T12:34:05.352863Z",
     "start_time": "2025-12-30T12:34:04.243123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = str(config.GPT2_PATH)\n",
    "reward_model = GPT2RewardModel(model_name)\n",
    "reward_model.load_state_dict(torch.load(str(config.REWARD_MODEL_PATH), map_location='cpu'))"
   ],
   "id": "78727e15c1103c92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:34:08.727264Z",
     "start_time": "2025-12-30T12:34:08.681952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import ModelForCausalLMWithValueHead\n",
    "\n",
    "model_path = str(config.GPT2_SFT_PATH)\n",
    "model = ModelForCausalLMWithValueHead(model_path)"
   ],
   "id": "f9121c76ff87c79f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:34:42.548166Z",
     "start_time": "2025-12-30T12:34:42.418844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(str(config.SST2_PATH))\n",
    "print(dataset)\n",
    "\n",
    "ds_train, ds_val = dataset['train'], dataset['validation']"
   ],
   "id": "bc2458dffa8d72d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:34:45.965912Z",
     "start_time": "2025-12-30T12:34:45.842662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(ds_train))\n",
    "ds_train = ds_train.filter(lambda x: len(x['sentence'].split(' ')) > 8)\n",
    "ds_val = ds_val.filter(lambda x: len(x['sentence'].split(' ')) > 8)\n",
    "\n",
    "print(len(ds_train))\n",
    "print(len(ds_val))"
   ],
   "id": "30f2a97fe9ddf6b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 67349/67349 [00:00<00:00, 621088.39 examples/s]\n",
      "Filter: 100%|██████████| 872/872 [00:00<00:00, 218654.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31105\n",
      "807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:34:50.422354Z",
     "start_time": "2025-12-30T12:34:50.419486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "input_min_token_length = 2\n",
    "input_max_token_length = 8\n",
    "input_token_length_range = list(range(\n",
    "    input_min_token_length,\n",
    "    input_max_token_length))\n",
    "print(input_token_length_range)\n",
    "print(random.choice(input_token_length_range))"
   ],
   "id": "802286060b74d9ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7]\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:34:55.726479Z",
     "start_time": "2025-12-30T12:34:54.006938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(sample):\n",
    "    input_size = random.choice(input_token_length_range)\n",
    "    sample['input_ids'] = tokenizer.encode(sample['sentence'])[:input_size]\n",
    "    sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
    "    sample['query'] = tokenizer.decode(sample['input_ids'])\n",
    "    return sample\n",
    "\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": False,\n",
    "    \"remove_columns\": ['idx', 'sentence', 'label']\n",
    "}\n",
    "\n",
    "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
    "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)"
   ],
   "id": "9e9ca36fdc482584",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31105/31105 [00:01<00:00, 18974.49 examples/s]\n",
      "Map: 100%|██████████| 807/807 [00:00<00:00, 16626.24 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:35:05.992043Z",
     "start_time": "2025-12-30T12:35:05.984940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset_train.set_format(type='torch')\n",
    "tokenized_dataset_val.set_format(type='torch')\n",
    "\n",
    "print(tokenized_dataset_train[6])"
   ],
   "id": "ef326cca2cf92261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 1640,   883,  3807, 31006,   508, 13121]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1]), 'query': 'for those moviegoers who complain'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:35:09.321570Z",
     "start_time": "2025-12-30T12:35:09.319023Z"
    }
   },
   "cell_type": "code",
   "source": "REWARD_TOKEN_ID = tokenizer.eos_token_id",
   "id": "6cd8634534423699",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:35:12.784994Z",
     "start_time": "2025-12-30T12:35:12.775364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    return dict((key, [d[key] for d in batch]) for key in batch[0])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset_train, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
    "val_dataloader = DataLoader(tokenized_dataset_val, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)"
   ],
   "id": "5cf78deb525b7917",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [tensor([ 270,  705,   82, 4071,  284, 1064]), tensor([  272, 14274]), tensor([ 265, 8699]), tensor([24267, 19522]), tensor([ 271,  326,  607, 6628]), tensor([18302, 11184,   284,  1416,    64,   705,    82]), tensor([270, 705,  82]), tensor([ 9930,  2499, 15013,   306,   880]), tensor([  301,  2645, 16772,   837,   262,  3807,   318]), tensor([ 505, 1917,  351,  262, 3807]), tensor([  325,   368,   284,   307, 18951, 13658]), tensor([5171,  691, 2148,  340,  351,  523]), tensor([ 5661,   318, 18700, 49291,   837]), tensor([27144,   452,   689,   290,  2523,   703]), tensor([   64,  4601,    88,    12, 34670]), tensor([1169, 2761,  290, 3435,  340]), tensor([   11,   290,   511, 20929, 17777,  7702]), tensor([   64,  4713,   837, 17774, 10997,   326]), tensor([  505,   286,   995, 22041,   705,    82,   749]), tensor([ 1169, 10092, 11375,   326,   739]), tensor([  65, 1044,  306]), tensor([ 3919, 15119,   994]), tensor([5171,  991,  307]), tensor([  325,  5431,   837, 19501,   290]), tensor([50042,   340,   318,   588,   852, 13640,   379]), tensor([  620, 17974,   663,  1388, 10039,  9432]), tensor([11261,  5879,  1165, 47370]), tensor([10134,   257,   835,   286,   384,  7213]), tensor([ 4480,  1811, 13644,   837,  3025]), tensor([   79, 49809,   318,   257]), tensor([ 1169,  4417,  1554,    81, 49900]), tensor([1462,  262, 5259,  934])], 'attention_mask': [tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1]), tensor([1, 1, 1]), tensor([1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1])], 'query': [\"it 's rare to find\", 'an unh', 'at 78', 'goldmember', 'is that her confidence', \"preserves tosca 's\", \"it 's\", 'they works spectacularly well', 'stylistically , the movie is', 'one problem with the movie', 'seem to be introverted', 'can only provide it with so', 'this is lightweight filmmaking ,', 'captivates and shows how', 'a wishy-wash', 'the problems and characters it', ', and their personalities undergo radical', 'a fresh , entertaining comedy that', \"one of world cinema 's most\", 'the dramatic conviction that under', 'blandly', 'no psychology here', 'can still be', 'sexy , peculiar and', 'watching it is like being trapped at', 'achieves its main strategic objective', 'may prove too convoluted', 'has a way of seeping', 'with several survivors , whose', 'possession is a', 'the surface histrionics', 'to the titular']}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:35:19.430118Z",
     "start_time": "2025-12-30T12:35:19.427385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_min_length = 5\n",
    "output_max_length = 16\n",
    "\n",
    "# https://huggingface.co/docs/trl/how_to_train#how-to-generate-text-for-training\n",
    "# gpt2-sft输出的配置\n",
    "# - 模型会从整个词汇表中按照原始概率分布进行采样\n",
    "# - 每个词被选中的概率完全由模型的原始输出决定\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,  # 所有词汇表中的词都可能被选中\n",
    "    \"top_p\": 1.0,  # 包含整个概率分布\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id\n",
    "}"
   ],
   "id": "3289da8b0aef6db0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:35:46.263079Z",
     "start_time": "2025-12-30T12:35:46.252976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
    "generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
    "sample = tokenizer('Hi, this')\n",
    "print(sample)"
   ],
   "id": "45f56476909409e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [17250, 11, 428], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:35:50.302963Z",
     "start_time": "2025-12-30T12:35:49.464729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_response = model.generate(\n",
    "    input_ids=torch.tensor(sample['input_ids']).unsqueeze(0),\n",
    "    attention_mask=torch.tensor(sample['attention_mask']).unsqueeze(0),\n",
    "    **generation_kwargs\n",
    ").squeeze(0)\n",
    "print(query_response)"
   ],
   "id": "8fedb412a0cec55d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17250,    11,   428,   318,   257,  3807,   326,  7138, 23007,   262,\n",
      "        13843,   286])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:35:55.296841Z",
     "start_time": "2025-12-30T12:35:55.293147Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(query_response))",
   "id": "5bf9ffabe842ba4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, this is a movie that perfectly captures the imagination of\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:36:15.843971Z",
     "start_time": "2025-12-30T12:36:15.690207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    query_response_score = torch.cat([\n",
    "        query_response,\n",
    "        torch.tensor([REWARD_TOKEN_ID])])\n",
    "    attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
    "    score = reward_model(\n",
    "        query_response_score.unsqueeze(0),\n",
    "        attention_mask.unsqueeze(0)\n",
    "    ).squeeze(0)[-1]\n",
    "print(score)"
   ],
   "id": "a7efd0426ba34f78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9988)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:36:29.334137Z",
     "start_time": "2025-12-30T12:36:24.992998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "reward_model = reward_model.to(device)\n",
    "\n",
    "query_tensors = batch['input_ids']\n",
    "query_attention_masks = batch['attention_mask']\n",
    "\n",
    "response_tensors = []\n",
    "query_response_tensors = []\n",
    "score_tensors = []\n",
    "\n",
    "for i, query in enumerate(query_tensors):\n",
    "    query = query.to(device)\n",
    "    query_attention_mask = query_attention_masks[i].to(device)\n",
    "    new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
    "    generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
    "    query_response = model.generate(\n",
    "        input_ids=query.unsqueeze(0),\n",
    "        attention_mask=query_attention_mask.unsqueeze(0),\n",
    "        **generation_kwargs\n",
    "    ).squeeze(0)\n",
    "\n",
    "    response_len = len(query_response) - len(query)\n",
    "    response_tensors.append(query_response[-response_len:])\n",
    "    query_response_tensors.append(query_response)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
    "        attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
    "        score = reward_model(\n",
    "            query_response_score.unsqueeze(0),\n",
    "            attention_mask.unsqueeze(0)\n",
    "        ).squeeze(0)[-1]\n",
    "        score = 2 * (score - 0.5)\n",
    "    score_tensors.append(score)\n",
    "\n",
    "batch[\"response\"] = [tokenizer.decode(response) for response in response_tensors]\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(batch['response'])"
   ],
   "id": "9aecd01ff3dafe90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an entire country that embraces the measure of maturity iced tea descends',\n",
      " 'oly mess   ',\n",
      " ' minutes long , this movie',\n",
      " ' is simply overpowered .     ',\n",
      " ' grows with every passing frame      ',\n",
      " ' upscale , offensive tone throughout but icky , pot-',\n",
      " ' sexier and boldier',\n",
      " ' in this regard .       ',\n",
      " ' flat and unconvincing ',\n",
      " \" 's direction      ly\",\n",
      " ' enough to tolerate a fetish iced tea party iced tea party ,',\n",
      " ' much sympathy , that it is impossible',\n",
      " \" and ian park jones 's veracity\",\n",
      " ' willing the audience was to speak out while others in',\n",
      " 'y kiddie flick iced',\n",
      " ' explores \\xa0are rather mind bogg',\n",
      " ' changes about every single aspect of them , both',\n",
      " ' manages to make a point .',\n",
      " ' ironic gifted artists ian holm ian freeman does just that',\n",
      " \"lies the film 's wonderfully shaped narrative \",\n",
      " ' and tragic ética   iken     ike',\n",
      " ' .     I might have been disappointed',\n",
      " ' a leap of faith in an era of mergers and downs',\n",
      " ' weirdly shapable ',\n",
      " ' an amusement park waiting to',\n",
      " ' : to create a film that people will buy and pay for .  ',\n",
      " ' to be clever and distracted , as it often takes the form of cheesy sub',\n",
      " ' into your consciousness , letting you',\n",
      " ' glee runs red in the face .\")    ',\n",
      " ' powerful feminist documentary that examines after the battle to hold our '\n",
      " 'society hostage . ',\n",
      " ' are puerile , stilted and downright creepy',\n",
      " ' hero , audiences will not']\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:39:07.480587Z",
     "start_time": "2025-12-30T12:39:07.340272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "sft_model = deepcopy(model)"
   ],
   "id": "a242894d726a07ae",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:39:11.684834Z",
     "start_time": "2025-12-30T12:39:11.648136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "input_data = data_collator([\n",
    "    {'input_ids': ids,\n",
    "     'attention_mask': torch.ones_like(ids)} for ids in query_response_tensors\n",
    "]).to(device)\n",
    "print(input_data)"
   ],
   "id": "50162c8a0a10ae14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  270,   705,    82,  4071,   284,  1064,   281,  2104,  1499,   326,\n",
      "         37872,   262,  3953,   286, 24841,   220,  3711,  8887,  1715,  2412,\n",
      "         50256],\n",
      "        [  272, 14274,  3366,  2085,   220,   220,   220, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  265,  8699,  2431,   890,   837,   428,  3807, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [24267, 19522,   318,  2391, 49313,   764,   220,   220,   220,   220,\n",
      "           220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  271,   326,   607,  6628, 13676,   351,   790,  6427,  5739,   220,\n",
      "           220,   220,   220,   220,   220, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [18302, 11184,   284,  1416,    64,   705,    82, 44918,   837,  5859,\n",
      "          8216,  3690,   475,   220, 17479,   837,  1787,    12, 50256, 50256,\n",
      "         50256],\n",
      "        [  270,   705,    82,  1714,   959,   290, 10758,   959, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 9930,  2499, 15013,   306,   880,   287,   428,  2754,   764,   220,\n",
      "           220,   220,   220,   220,   220,   220, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  301,  2645, 16772,   837,   262,  3807,   318,  6228,   290, 21254,\n",
      "            85,  1939,   278,   220, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  505,  1917,   351,   262,  3807,   705,    82,  4571,   220,   220,\n",
      "           220,   220,   220,   220,   306, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  325,   368,   284,   307, 18951, 13658,  1576,   284, 21923,   257,\n",
      "         32870,   220,  3711,  8887,  2151,   220,  3711,  8887,  2151,   837,\n",
      "         50256],\n",
      "        [ 5171,   691,  2148,   340,   351,   523,   881, 20242,   837,   326,\n",
      "           340,   318,  5340, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 5661,   318, 18700, 49291,   837,   290,   220,   666,  3952,   474,\n",
      "          1952,   705,    82,  3326,  4355, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [27144,   452,   689,   290,  2523,   703,  4684,   262,  5386,   373,\n",
      "           284,  2740,   503,   981,  1854,   287, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   64,  4601,    88,    12, 34670,    88,   479,  1638,   494, 26810,\n",
      "           220,  3711, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 1169,  2761,   290,  3435,   340, 25409,   220,  1849,   533,  2138,\n",
      "          2000,   275, 10332, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   11,   290,   511, 20929, 17777,  7702,  2458,   546,   790,  2060,\n",
      "          4843,   286,   606,   837,  1111, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   64,  4713,   837, 17774, 10997,   326, 15314,   284,   787,   257,\n",
      "           966,   764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  505,   286,   995, 22041,   705,    82,   749, 25304, 22527,  7912,\n",
      "           220,   666,  6039,    76,   220,   666,  2030,  8463,   857,   655,\n",
      "           326],\n",
      "        [ 1169, 10092, 11375,   326,   739, 13508,   262,  2646,   705,    82,\n",
      "         33138, 14292,  8689,   220, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   65,  1044,   306,   290, 15444,   220, 25125,  3970,   220,   220,\n",
      "           220, 29943,   220,   220,   220,   220,   220,   522, 50256, 50256,\n",
      "         50256],\n",
      "        [ 3919, 15119,   994,   764,   220,   220,   220,   220,   314,  1244,\n",
      "           423,   587, 11679, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 5171,   991,   307,   257, 16470,   286,  4562,   287,   281,  6980,\n",
      "           286,  4017,  5355,   290, 21838, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  325,  5431,   837, 19501,   290,  7650,   306,   427,   499,   540,\n",
      "           220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [50042,   340,   318,   588,   852, 13640,   379,   281, 30184,  3952,\n",
      "          4953,   284, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  620, 17974,   663,  1388, 10039,  9432,  1058,   284,  2251,   257,\n",
      "          2646,   326,   661,   481,  2822,   290,  1414,   329,   764,   220,\n",
      "           220],\n",
      "        [11261,  5879,  1165, 47370,   284,   307, 14169,   290, 22943,   837,\n",
      "           355,   340,  1690,  2753,   262,  1296,   286, 45002,   850, 50256,\n",
      "         50256],\n",
      "        [10134,   257,   835,   286,   384,  7213,   656,   534, 10510,   837,\n",
      "          9616,   345, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 4480,  1811, 13644,   837,  3025,   308,  7197,  4539,  2266,   287,\n",
      "           262,  1986,   220, 19570,   220,   220,   220,   220, 50256, 50256,\n",
      "         50256],\n",
      "        [   79, 49809,   318,   257,  3665, 14314, 11648,   326, 28025,   706,\n",
      "           262,  3344,   284,  1745,   674,  3592, 23229,   764,   220, 50256,\n",
      "         50256],\n",
      "        [ 1169,  4417,  1554,    81, 49900,   389,   279, 15573,   576,   837,\n",
      "           336,   346,  1513,   290, 31464, 23387, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 1462,   262,  5259,   934,  4293,   837, 15579,   481,   407, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:39:24.668557Z",
     "start_time": "2025-12-30T12:39:24.662194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_rewards(\n",
    "        input_data,\n",
    "        query_tensors,\n",
    "        response_tensors,\n",
    "        score_tensors\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        # 正在微调的模型所输出的token的logits和token的价值\n",
    "        # 模型输出所有token的概率分布\n",
    "        logits, values = model(**input_data)  # b, seq, vocab\n",
    "        # 冻结的模型的输出和价值\n",
    "        ref_logits, _ = sft_model(**input_data)\n",
    "        # 正在微调的模型的输出的对数概率\n",
    "        logp = torch.nn.functional.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "        # 冻结的模型的输出的对数概率\n",
    "        ref_logp = torch.nn.functional.log_softmax(ref_logits[:, :-1, :], dim=-1)\n",
    "        # 实际生成的token序列\n",
    "        labels = input_data['input_ids'][:, 1:]  # b, seq\n",
    "        # 使用gather提取实际token的概率\n",
    "        logp = torch.gather(\n",
    "            logp,\n",
    "            2,\n",
    "            labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)  # batch, seq\n",
    "        ref_logp = torch.gather(\n",
    "            ref_logp,\n",
    "            2,\n",
    "            labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)  # batch, seq\n",
    "        # kl散度\n",
    "        kl = logp - ref_logp\n",
    "        # kl散度的权重\n",
    "        beta = 0.2\n",
    "        # 最终奖励的计算\n",
    "        rewards = - beta * kl\n",
    "        attention_mask = input_data['attention_mask']\n",
    "        masks = torch.zeros_like(attention_mask[:, 1:])\n",
    "        masks[:, :] = attention_mask[:, 1:]\n",
    "        for j in range(len(query_tensors)):\n",
    "            start = len(query_tensors[j]) - 1\n",
    "            end = start + len(response_tensors[j])\n",
    "            masks[j, :start] = 0\n",
    "            masks[j, end:] = 0\n",
    "            rewards[j, end - 1] += score_tensors[j]\n",
    "            rewards[j, :] *= masks[j, :]\n",
    "            values[j, :-1] *= masks[j, :]\n",
    "\n",
    "    return logp, rewards, values[:, :-1], masks"
   ],
   "id": "3f5fba14e372532c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:40:51.446121Z",
     "start_time": "2025-12-30T12:40:50.756100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logprobs, rewards, values, masks = compute_rewards(\n",
    "    input_data,\n",
    "    query_tensors,\n",
    "    response_tensors,\n",
    "    score_tensors\n",
    ")\n",
    "print(rewards[0])\n",
    "print(input_data['input_ids'][0])\n",
    "print(input_data['attention_mask'][0])\n",
    "print(masks[0])\n",
    "print(values[0])"
   ],
   "id": "2dd830585b0b48d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        0.9417, -0.0000])\n",
      "tensor([  270,   705,    82,  4071,   284,  1064,   281,  2104,  1499,   326,\n",
      "        37872,   262,  3953,   286, 24841,   220,  3711,  8887,  1715,  2412,\n",
      "        50256])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9542, -0.0225,  0.7976,\n",
      "         0.5902,  1.7268,  3.9876, -0.0290,  1.9629,  2.6377,  1.9755,  0.1747,\n",
      "         1.0370,  2.4842, -0.6464,  0.0000])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:41:25.454667Z",
     "start_time": "2025-12-30T12:41:25.448313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def masked_mean(values, mask):\n",
    "    # 计算带掩码的平均值\n",
    "    return (values * mask).sum() / mask.sum()\n",
    "\n",
    "\n",
    "def masked_var(values, mask):\n",
    "    # 计算带掩码的方差\n",
    "    mean = masked_mean(values, mask)\n",
    "    centred_values = values - mean\n",
    "    return masked_mean(centred_values ** 2, mask)\n",
    "\n",
    "\n",
    "def masked_whiten(values, mask):\n",
    "    '''\n",
    "    对数据进行带掩码的白化处理，\n",
    "    让有效数据的方差变为1，但均值保持不变\n",
    "    '''\n",
    "    mean, var = masked_mean(values, mask), masked_var(values, mask)\n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "    whitened += mean\n",
    "    return whitened\n",
    "\n",
    "\n",
    "def compute_advantage(rewards, values, masks):\n",
    "    '''\n",
    "    广义优势估计（GAE）\n",
    "    '''\n",
    "    lastgae = 0.0\n",
    "    advantage_reversed = []\n",
    "    seq_length = rewards.shape[-1]\n",
    "    gamma, lam = 1.0, 0.95\n",
    "\n",
    "    for t in reversed(range(seq_length)):\n",
    "        nextvalues = values[:, t + 1] if t < seq_length - 1 else 0.0\n",
    "        delta = rewards[:, t] + gamma * nextvalues - values[:, t]\n",
    "        lastgae = delta + gamma * lam * lastgae\n",
    "        advantage_reversed.append(lastgae)\n",
    "    advantages = torch.stack(advantage_reversed[::-1], dim=1)\n",
    "    advantages = masked_whiten(advantages, masks)\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ],
   "id": "24884996664563e6",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:41:38.443520Z",
     "start_time": "2025-12-30T12:41:38.433670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "advantages, returns = compute_advantage(rewards, values, masks)\n",
    "print(advantages[0])\n",
    "print(returns[0])"
   ],
   "id": "6cdab9a1fcce5b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2937,  0.3265,  0.3611,  0.3975,  0.4358, -0.9009,  0.4619, -0.0743,\n",
      "         0.0853, -0.6937, -2.3060,  0.4204, -0.9437, -1.4515, -1.0439,  0.1875,\n",
      "        -0.3929, -1.4160,  0.7329, -0.3302])\n",
      "tensor([ 0.2937,  0.3265,  0.3611,  0.3975,  0.4358,  1.0532,  0.4394,  0.7233,\n",
      "         0.6755,  1.0331,  1.6816,  0.3914,  1.0192,  1.1862,  0.9316,  0.3622,\n",
      "         0.6441,  1.0682,  0.0865, -0.3302])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:44:47.699770Z",
     "start_time": "2025-12-30T12:44:47.689351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# 随机排列一下各个批次大小\n",
    "np.random.permutation(batch_size)"
   ],
   "id": "fe3368ad78b0082e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 13,  4, 21, 18, 29, 30, 14,  7,  5,  0, 12,  1, 16,  8, 26, 22,\n",
       "       23, 24,  2, 11, 20, 15, 27,  3, 19, 31,  6, 25, 10, 17,  9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:45:03.542825Z",
     "start_time": "2025-12-30T12:45:03.535143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 最小的批次大小\n",
    "mini_batch_size = 4\n",
    "# 训练 4 个 epoch\n",
    "ppo_epochs = 4\n",
    "# ε = 0.2\n",
    "cliprange_ratio = 0.2\n",
    "\n",
    "v_loss_coeff = 0.1\n",
    "# 比例的阈值\n",
    "ratio_threshold = 10\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "        old_logprobs,\n",
    "        values,\n",
    "        logprobs,\n",
    "        vpreds,\n",
    "        masks,\n",
    "        advantages,\n",
    "        returns\n",
    "):\n",
    "    ratio = torch.exp(logprobs - old_logprobs)\n",
    "    pg_loss1 = - ratio * advantages\n",
    "    pg_loss2 = - torch.clamp(\n",
    "        ratio,\n",
    "        1 - cliprange_ratio,\n",
    "        1 + cliprange_ratio\n",
    "    ) * advantages\n",
    "    pg_loss = masked_mean(torch.max(pg_loss1, pg_loss2), masks)\n",
    "\n",
    "    v_loss = masked_mean((vpreds - returns) ** 2, masks)\n",
    "    loss = pg_loss + v_loss_coeff * v_loss\n",
    "\n",
    "    avg_ratio = masked_mean(ratio, masks)\n",
    "    if avg_ratio > ratio_threshold:\n",
    "        pg_loss = pg_loss * 0.0\n",
    "        v_loss = v_loss * 0.0\n",
    "        loss = loss * 0.0\n",
    "\n",
    "    return loss, v_loss\n",
    "\n",
    "\n",
    "def mini_batch_train():\n",
    "    # 过滤掉输入数据为空的批次\n",
    "    if input_data['input_ids'].shape[0] == 0:\n",
    "        return\n",
    "    for ep in range(ppo_epochs):\n",
    "        batch_inds = np.random.permutation(batch_size)\n",
    "\n",
    "        for start in range(0, batch_size, mini_batch_size):\n",
    "            end = start + mini_batch_size\n",
    "            mini_batch_inds = batch_inds[start:end]\n",
    "\n",
    "            mb_model_inputs = {\n",
    "                'input_ids': input_data['input_ids'][mini_batch_inds],\n",
    "                'attention_mask': input_data['attention_mask'][mini_batch_inds]\n",
    "            }\n",
    "            mb_logits, mb_vpreds = model(**mb_model_inputs)\n",
    "            mb_logits = torch.nn.functional.log_softmax(\n",
    "                mb_logits[:, :-1, :],\n",
    "                dim=-1\n",
    "            )\n",
    "            mb_logprobs = torch.gather(\n",
    "                mb_logits,\n",
    "                2,\n",
    "                mb_model_inputs['input_ids'][:, 1:].unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            loss, loss_v = compute_loss(\n",
    "                logprobs[mini_batch_inds],\n",
    "                values[mini_batch_inds],\n",
    "                mb_logprobs,\n",
    "                mb_vpreds[:, :-1],\n",
    "                masks[mini_batch_inds],\n",
    "                advantages[mini_batch_inds],\n",
    "                returns[mini_batch_inds]\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('loss/total', loss.item())\n",
    "    print('mini-batch training finished')"
   ],
   "id": "7fc19801b1477566",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:45:29.919265Z",
     "start_time": "2025-12-30T12:45:16.832279Z"
    }
   },
   "cell_type": "code",
   "source": "mini_batch_train()",
   "id": "9596469d5f17d3f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/total 1.727279543876648\n",
      "loss/total 0.7686131596565247\n",
      "loss/total 1.262438416481018\n",
      "loss/total 1.662282943725586\n",
      "loss/total 1.1627838611602783\n",
      "loss/total 0.7339403033256531\n",
      "loss/total 1.5563760995864868\n",
      "loss/total 0.7673659324645996\n",
      "loss/total 1.1146233081817627\n",
      "loss/total 0.8554384708404541\n",
      "loss/total 0.7849952578544617\n",
      "loss/total 0.8114762306213379\n",
      "loss/total 0.8627503514289856\n",
      "loss/total 0.8448833227157593\n",
      "loss/total 1.2873870134353638\n",
      "loss/total 1.0911635160446167\n",
      "loss/total 0.7411577105522156\n",
      "loss/total 0.9025058746337891\n",
      "loss/total 0.7879412770271301\n",
      "loss/total 0.4668281674385071\n",
      "loss/total 1.384494423866272\n",
      "loss/total 0.7904667854309082\n",
      "loss/total 1.183447003364563\n",
      "loss/total 1.2399688959121704\n",
      "loss/total 1.110142469406128\n",
      "loss/total 0.3123805820941925\n",
      "loss/total 0.7012345790863037\n",
      "loss/total 1.2559528350830078\n",
      "loss/total 0.9507017135620117\n",
      "loss/total 0.4577327072620392\n",
      "loss/total 0.9585307240486145\n",
      "loss/total 1.0084588527679443\n",
      "mini-batch training finished\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # Generate responses\n",
    "        query_tensors = batch['input_ids']\n",
    "        query_attention_masks = batch['attention_mask']\n",
    "\n",
    "        response_tensors = []\n",
    "        query_response_tensors = []\n",
    "        score_tensors = []\n",
    "\n",
    "        for i, query in enumerate(query_tensors):\n",
    "            query = query.to(device)\n",
    "            query_attention_mask = query_attention_masks[i].to(device)\n",
    "            new_tokens = random.choice(list(range(\n",
    "                output_min_length,\n",
    "                output_max_length)))\n",
    "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
    "            query_response = model.generate(\n",
    "                input_ids=query.unsqueeze(0),\n",
    "                attention_mask=query_attention_mask.unsqueeze(0),\n",
    "                **generation_kwargs\n",
    "            ).squeeze(0)\n",
    "\n",
    "            response_len = len(query_response) - len(query)\n",
    "            response_tensors.append(query_response[-response_len:])\n",
    "            query_response_tensors.append(query_response)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                query_response_score = torch.cat([\n",
    "                    query_response,\n",
    "                    torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
    "                attention_mask = torch.ones_like(\n",
    "                    query_response_score,\n",
    "                    dtype=torch.long)\n",
    "                score = reward_model(\n",
    "                    query_response_score.unsqueeze(0),\n",
    "                    attention_mask.unsqueeze(0)\n",
    "                ).squeeze(0)[-1]\n",
    "                score = 2 * (score - 0.5)\n",
    "            score_tensors.append(score)\n",
    "\n",
    "        input_data = data_collator([\n",
    "            {\n",
    "                'input_ids': ids,\n",
    "                'attention_mask': torch.ones_like(ids)\n",
    "            }\n",
    "            for ids in query_response_tensors\n",
    "        ]).to(device)\n",
    "\n",
    "        # 奖励和优势\n",
    "        logprobs, rewards, values, masks = compute_rewards(\n",
    "            input_data,\n",
    "            query_tensors,\n",
    "            response_tensors,\n",
    "            score_tensors\n",
    "        )\n",
    "        advantages, returns = compute_advantage(rewards, values, masks)\n",
    "\n",
    "        # 小批次训练\n",
    "        mini_batch_train()\n",
    "    print(f'epoch {epoch + 1} finished')"
   ],
   "id": "e9cec20f5856ae51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T13:40:16.938760Z",
     "start_time": "2025-12-30T13:40:16.931567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(tokenized_dataset_val))\n",
    "val_gen_lengths = [0] * len(tokenized_dataset_val)\n",
    "for i in range(len(tokenized_dataset_val)):\n",
    "    val_gen_lengths[i] = random.choice(list(range(\n",
    "        output_min_length,\n",
    "        output_max_length)))\n",
    "val_gen_lengths[:10]"
   ],
   "id": "f3b514f4e7eb2164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 8, 15, 12, 14, 5, 5, 9, 9, 7]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T13:40:21.789250Z",
     "start_time": "2025-12-30T13:40:21.783383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model):\n",
    "    scores = []\n",
    "    for b, batch in enumerate(val_dataloader):\n",
    "        # Generate_responses\n",
    "        query_tensors = batch['input_ids']\n",
    "        query_attention_masks = batch['attention_mask']\n",
    "        for i, query in enumerate(query_tensors):\n",
    "            query = query.to(device)\n",
    "            query_attention_mask = query_attention_masks[i].to(device)\n",
    "            new_tokens = val_gen_lengths[b * len(query_tensors) + i]\n",
    "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
    "            query_response = model.generate(\n",
    "                input_ids=query.unsqueeze(0),\n",
    "                attention_mask=query_attention_mask.unsqueeze(0),\n",
    "                **generation_kwargs\n",
    "            ).squeeze(0)\n",
    "            query_response_score = torch.cat([\n",
    "                query_response,\n",
    "                torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
    "            attention_mask = torch.ones_like(\n",
    "                query_response_score, dtype=torch.long)\n",
    "            score = reward_model(\n",
    "                query_response_score.unsqueeze(0),\n",
    "                attention_mask.unsqueeze(0)\n",
    "            ).squeeze(0)[-1]\n",
    "            score = 2 * (score - 0.5)\n",
    "            scores.append(score.item())\n",
    "    print('平均分数:', sum(scores) / len(scores))"
   ],
   "id": "54a66fd99c1296d7",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T13:42:26.812679Z",
     "start_time": "2025-12-30T13:40:32.091747Z"
    }
   },
   "cell_type": "code",
   "source": "validate(model)",
   "id": "4a5af27b0fb76309",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均分数: 0.5725250226414337\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T13:42:33.549801Z",
     "start_time": "2025-12-30T13:42:32.574910Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), str(config.PPO_MODEL_PATH))",
   "id": "eace4fc801e584ce",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T13:44:27.711498Z",
     "start_time": "2025-12-30T13:42:34.962022Z"
    }
   },
   "cell_type": "code",
   "source": "validate(sft_model)",
   "id": "e10bc1e9422f1bb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均分数: 0.11989582124105026\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
