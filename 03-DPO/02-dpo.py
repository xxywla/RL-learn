import time
from dataclasses import dataclass

import datasets
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

import config


@dataclass
class DPOConfig:
    max_length: int = 1700  # æ ¹æ®è‡ªèº«å…·å¤‡çš„ç®—åŠ›æ¡ä»¶è¿›è¡Œè‡ªé€‚åº”æ›´æ”¹
    batch_size: int = 2
    gradient_accumulation_steps: int = 8
    beta: float = 0.5  # Î²æ˜¯dpoå…¬å¼ä¸­çš„è¶…å‚æ•°
    log_iter: int = 200
    max_lr: float = 1e-6
    min_lr: float = 1e-7
    warmup_steps: int = 300


def tokenize_and_format(data):
    input_ids = tokenizer.apply_chat_template(
        data,
        tokenize=True,
        add_generation_prompt=False,
        truncation=True,
        max_length=DPOConfig.max_length,
    )

    return input_ids


# å®šä¹‰ä¸€ä¸ªæ—¥å¿—è®°å½•å‡½æ•°
def log_call(iters, iters_average_loss):
    with open(str(config.QWEN2_5_0_5B_DPO_LOG_PATH), "a") as my_file:
        my_file.write(
            f'time:{time.strftime("%Y-%m-%d, %H:%M:%S")},iters:{iters + 1}, iters_average_Loss:{iters_average_loss:.4f}\n')


def linear_warmup(current_step, warmup_steps, max_lr):
    if current_step < warmup_steps:
        return max_lr * current_step / warmup_steps
    else:
        return max_lr


def cosine_decay(current_step, warmup_steps, total_steps, max_lr, min_lr):
    if current_step < warmup_steps:
        return linear_warmup(current_step, warmup_steps, max_lr)
    else:
        progress = (current_step - warmup_steps) / (total_steps - warmup_steps)
        decay = 0.5 * (1 + np.cos(np.pi * progress))
        return (max_lr - min_lr) * decay + min_lr


def create_answer_mask(input_ids, tokenizer):
    """
    åˆ›å»ºä»…å¯¹åŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¡ç®—æŸå¤±çš„æ©ç 

    Args:
        input_ids: è¾“å…¥tokenåºåˆ— [batch_size, seq_len]
        tokenizer: åˆ†è¯å™¨

    Returns:
        answer_mask: åŠ©æ‰‹å›ç­”éƒ¨åˆ†ä¸º1ï¼Œå…¶ä»–éƒ¨åˆ†ä¸º0çš„æ©ç 
    """
    batch_size, seq_len = input_ids.shape
    answer_mask = torch.zeros_like(input_ids)

    # è·å–<im_end>æ ‡è®°çš„token_id
    eos_token_id = tokenizer.encode('<|im_end|>')[0]

    for batch_idx in range(batch_size):
        # æ‰¾åˆ°æ‰€æœ‰ <|im_end|> çš„ä½ç½®
        eos_positions = torch.where(
            input_ids[batch_idx] == eos_token_id
        )[0].tolist()

        if len(eos_positions) < 2:  # è‡³å°‘éœ€è¦userå’Œassistantå„ä¸€ä¸ªç»“æŸæ ‡è®°
            continue

        # è§£æå¯¹è¯è½®æ¬¡
        user_ends, assistant_ends = _parse_conversation_turns(eos_positions)

        # ä¸ºæ¯ä¸ªåŠ©æ‰‹å›ç­”è®¾ç½®æ©ç 
        _set_answer_masks(
            answer_mask[batch_idx],
            user_ends,
            assistant_ends,
            seq_len
        )

    return answer_mask


def _parse_conversation_turns(eos_positions):
    """
    è§£æå¯¹è¯è½®æ¬¡ï¼Œåˆ†ç¦»ç”¨æˆ·å’ŒåŠ©æ‰‹çš„ç»“æŸä½ç½®

    å¯¹è¯æ ¼å¼ï¼š
    <|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n{assistant_msg}<|im_end|>\n

    eos_positions[0]: systemç»“æŸ (å¦‚æœæœ‰)
    eos_positions[1]: ç¬¬1è½®userç»“æŸ
    eos_positions[2]: ç¬¬1è½®assistantç»“æŸ
    eos_positions[3]: ç¬¬2è½®userç»“æŸ
    eos_positions[4]: ç¬¬2è½®assistantç»“æŸ
    ...
    """
    # è·³è¿‡systeméƒ¨åˆ†ï¼Œä»ç¬¬ä¸€ä¸ªuserå¼€å§‹
    conversation_eos = eos_positions[1:]  # å»æ‰systemçš„<im_end>

    # å¶æ•°ç´¢å¼•ï¼šuserç»“æŸä½ç½®ï¼Œå¥‡æ•°ç´¢å¼•ï¼šassistantç»“æŸä½ç½®
    user_ends = [pos + 1 for pos in conversation_eos[::2]]  # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»0å¼€å§‹
    assistant_ends = [pos + 1 for pos in conversation_eos[1::2]]  # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»1å¼€å§‹

    return user_ends, assistant_ends


def _set_answer_masks(mask, user_ends, assistant_ends, seq_len):
    """
    ä¸ºåŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¾ç½®æ©ç 

    Args:
        mask: å½“å‰æ ·æœ¬çš„æ©ç  [seq_len]
        user_ends: ç”¨æˆ·æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨
        assistant_ends: åŠ©æ‰‹æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨
        seq_len: åºåˆ—é•¿åº¦
    """
    num_user_turns = len(user_ends)
    num_assistant_turns = len(assistant_ends)

    if num_user_turns == num_assistant_turns:
        # å®Œæ•´å¯¹è¯ï¼šæ¯è½®éƒ½æœ‰ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”
        for user_end, assistant_end in zip(user_ends, assistant_ends):
            answer_start = user_end + 3  # è·³è¿‡ <|im_start|>assistant\n
            answer_end = assistant_end - 1  # ä¸åŒ…å« <|im_end|>
            mask[answer_start:answer_end] = 1

    elif num_user_turns == num_assistant_turns + 1:
        # æœªå®Œæˆå¯¹è¯ï¼šæœ€åä¸€è½®åŠ©æ‰‹å›ç­”è¢«æˆªæ–­

        # å¤„ç†å®Œæ•´çš„å¯¹è¯è½®æ¬¡
        for user_end, assistant_end in zip(user_ends[:-1], assistant_ends):
            answer_start = user_end + 3
            answer_end = assistant_end - 1
            mask[answer_start:answer_end] = 1

        # å¤„ç†æœ€åä¸€è½®è¢«æˆªæ–­çš„åŠ©æ‰‹å›ç­”
        last_user_end = user_ends[-1]
        last_answer_start = last_user_end + 3
        mask[last_answer_start:] = 1  # åˆ°åºåˆ—ç»“å°¾


def _compute_average_log_probability(logits, target_labels, mask):
    """
    è®¡ç®—å¸¦æ©ç çš„å¹³å‡å¯¹æ•°æ¦‚ç‡

    Args:
        logits: æ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]
        target_labels: ç›®æ ‡æ ‡ç­¾ [batch_size, seq_len]
        mask: è®¡ç®—æ©ç  [batch_size, seq_len]

    Returns:
        average_log_prob: æ¯ä¸ªæ ·æœ¬çš„å¹³å‡å¯¹æ•°æ¦‚ç‡ [batch_size]
    """
    # è®¡ç®—softmaxæ¦‚ç‡åˆ†å¸ƒ
    probabilities = torch.softmax(logits, dim=-1)

    # è®¡ç®—å¯¹æ•°æ¦‚ç‡
    log_probabilities = torch.log(probabilities)

    # è·å–ç›®æ ‡tokençš„å¯¹æ•°æ¦‚ç‡
    gathered_log_probs = torch.gather(
        log_probabilities,
        dim=-1,
        index=target_labels.unsqueeze(2)
    ).squeeze(2)

    # åº”ç”¨æ©ç å¹¶è®¡ç®—å¹³å‡å€¼
    masked_log_probs = torch.mul(gathered_log_probs, mask)
    average_log_prob = masked_log_probs.sum(dim=-1) / mask.sum(dim=-1)

    return average_log_prob


if __name__ == '__main__':
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_path = str(config.QWEN2_5_0_5B_SFT_PATH)

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype="auto",
        device_map="auto"
    )
    # å†»ç»“çš„å‚è€ƒæ¨¡å‹
    ref_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype="auto",
        device_map="auto"
    )
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    print(model.generation_config)

    model.generation_config.do_sample = True
    model.generation_config.eos_token_id = [151645, 151643]
    model.generation_config.pad_token_id = 151643
    model.generation_config.temperature = 0.7
    model.generation_config.top_p = 0.8
    model.generation_config.top_k = 20
    model.generation_config.repetition_penalty = 1.05

    print(model.generation_config)

    binarized_data = datasets.load_dataset(str(config.ULTRA_FEEDBACK_PATH))

    ## ç”Ÿæˆåå¥½æ•°æ®çš„input_ids
    chosen_input_ids_list = []
    i = 0
    while True:
        data = binarized_data['train_sft'][i]['chosen']
        data.insert(
            0,
            {"content": "You are a helpful assistant", "role": "system"}
        )
        input_ids = tokenize_and_format(data)
        chosen_input_ids_list.append(input_ids)
        i += 1
        if i % 10000 == 0 or i == len(binarized_data['train_sft']):
            print(f"åå¥½æ•°æ®å·²å¤„ç†{i}æ¡æ•°æ®")
        if i == 30000:
            break
    print('-' * 70)

    #############################################################################
    ## ç”Ÿæˆä¸åå¥½æ•°æ®çš„input_ids
    rejected_input_ids_list = []
    i = 0
    while True:
        data = binarized_data['train_sft'][i]['rejected']
        data.insert(
            0,
            {"content": "You are a helpful assistant", "role": "system"}
        )
        input_ids = tokenize_and_format(data)
        rejected_input_ids_list.append(input_ids)
        i += 1
        if i % 10000 == 0 or i == len(binarized_data['train_sft']):
            print(f"éåå¥½æ•°æ®å·²å¤„ç†{i}æ¡æ•°æ®")
        if i == 30000:
            break

    ## ç¡®ä¿æ•°æ®æ¡æ•°ä¸€è‡´
    assert len(chosen_input_ids_list) == len(rejected_input_ids_list)

    beta = DPOConfig.beta  # Î²è¶…å‚æ•°
    batch_size = DPOConfig.batch_size
    gradient_accumulation_steps = DPOConfig.gradient_accumulation_steps
    log_iter = DPOConfig.log_iter
    max_lr = DPOConfig.max_lr
    min_lr = DPOConfig.min_lr
    warmup_steps = DPOConfig.warmup_steps
    total_steps = len(chosen_input_ids_list) // batch_size
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)
    trainable_parameters_num = sum(p.numel() for p in filter(lambda p: p.requires_grad, model.parameters()))  ##å…¨å‚å¾®è°ƒ
    print(f"è®­ç»ƒå‚æ•°æ•°é‡: {trainable_parameters_num}")

    with open(str(config.QWEN2_5_0_5B_DPO_LOG_PATH), "a") as my_file:
        my_file.write(
            f'time:{time.strftime("%Y-%m-%d, %H:%M:%S")}, batch_size:{batch_size}, trainable_parameters_num:{trainable_parameters_num},warmup_steps:{warmup_steps}, max_lr:{max_lr}, min_lr:{min_lr}\n')

    model.train()

    # ==================== è®­ç»ƒæŒ‡æ ‡è®°å½•åˆ—è¡¨ ====================
    training_losses = []
    # åå¥½çš„å›ç­”çš„æ¦‚ç‡
    preferred_log_probabilities = []
    # è®¨åŒçš„å›ç­”çš„æ¦‚ç‡
    rejected_log_probabilities = []
    # åå¥½çš„å›ç­”çš„å¥–åŠ±
    preferred_rewards = []
    # è®¨åŒçš„å›ç­”çš„å¥–åŠ±
    rejected_rewards = []
    reward_margins = []

    model.zero_grad()  # è®­ç»ƒå¼€å§‹æ—¶æ¸…ç©ºæ¢¯åº¦
    skipped_batches_count = 0
    total_batches = len(chosen_input_ids_list) // batch_size

    for batch_idx in range(total_batches):
        ## ==================== è·å–æ‰¹æ¬¡æ•°æ® ====================

        # è·å–å½“å‰æ‰¹æ¬¡çš„åå¥½å¯¹æ•°æ®
        preferred_batch_sequences = chosen_input_ids_list[
            batch_idx * batch_size:(batch_idx + 1) * batch_size
        ]
        rejected_batch_sequences = rejected_input_ids_list[
            batch_idx * batch_size:(batch_idx + 1) * batch_size
        ]

        ## ==================== æ•°æ®å¡«å……å¯¹é½ ====================

        # è®¡ç®—å„è‡ªæ‰¹æ¬¡çš„æœ€å¤§åºåˆ—é•¿åº¦
        preferred_max_length = max([len(sequence) for sequence in preferred_batch_sequences])
        rejected_max_length = max([len(sequence) for sequence in rejected_batch_sequences])
        # ä½¿ç”¨eos tokenä½œä¸ºpad token
        pad_token_id = model.generation_config.eos_token_id[-1]

        ### åå¥½æ•°æ®å¡«å……å¤„ç†
        preferred_padded_sequences = []
        for seq_idx in range(batch_size):
            original_sequence = preferred_batch_sequences[seq_idx]
            # è®¡ç®—è¦å¡«å……å¤šå°‘ä¸ªpad
            padding_length = preferred_max_length - len(original_sequence)
            # åœ¨è®­ç»ƒæ•°æ®çš„æœ«å°¾å¡«å……pad
            padded_sequence = torch.nn.functional.pad(
                torch.tensor(original_sequence),
                (0, padding_length),
                mode='constant',
                value=pad_token_id
            ).tolist()
            # å°†å¡«å……è¿‡çš„æ•°æ®æ”¾å…¥åˆ—è¡¨
            preferred_padded_sequences.append(padded_sequence)

        preferred_batch_tensor = torch.tensor(preferred_padded_sequences)

        ### æ‹’ç»æ•°æ®å¡«å……å¤„ç†
        rejected_padded_sequences = []
        for seq_idx in range(batch_size):
            original_sequence = rejected_batch_sequences[seq_idx]
            padding_length = rejected_max_length - len(original_sequence)

            padded_sequence = torch.nn.functional.pad(
                torch.tensor(original_sequence),
                (0, padding_length),
                mode='constant',
                value=pad_token_id
            ).tolist()

            rejected_padded_sequences.append(padded_sequence)

        rejected_batch_tensor = torch.tensor(rejected_padded_sequences)

        ## ==================== æ„å»ºè¾“å…¥è¾“å‡ºå¯¹ ====================

        # æ„å»ºå› æœè¯­è¨€æ¨¡å‹çš„è¾“å…¥è¾“å‡ºå¯¹ï¼šx->yï¼ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼‰
        # æ¨¡å‹çš„è¾“å…¥ï¼šåå¥½çš„å›ç­”
        preferred_model_inputs = preferred_batch_tensor[:, :-1].to(device)
        # çœŸå®çš„æ ‡ç­¾
        preferred_target_labels = preferred_batch_tensor[:, 1:].to(device)

        rejected_model_inputs = rejected_batch_tensor[:, :-1].to(device)
        rejected_target_labels = rejected_batch_tensor[:, 1:].to(device)

        ## ==================== æ„å»ºè®­ç»ƒæ©ç  ====================

        # æ„å»ºæ©ç çŸ©é˜µï¼špadding_maskï¼ˆå¿½ç•¥å¡«å……tokenï¼‰+ answer_maskï¼ˆåªå…³æ³¨å›ç­”éƒ¨åˆ†ï¼‰

        # pad_token_id å¯¹åº”çš„ç½®ä¸º 0 ï¼Œå…¶å®ƒç½®ä¸º 1 ã€‚
        preferred_padding_mask = torch.where(
            preferred_target_labels == pad_token_id,
            0,
            1
        )
        rejected_padding_mask = torch.where(
            rejected_target_labels == pad_token_id,
            0,
            1
        )

        # åŠ©æ‰‹å›ç­”çš„æ©ç ï¼šå°†åŠ©æ‰‹å›ç­”çš„éƒ¨åˆ†æ©ç ä¸º 1 ã€‚å…¶å®ƒéƒ½æ˜¯ 0 ã€‚
        preferred_answer_mask = create_answer_mask(
            preferred_model_inputs,
            tokenizer
        )
        rejected_answer_mask = create_answer_mask(
            rejected_model_inputs,
            tokenizer
        )

        # æœ€ç»ˆæ©ç ï¼šå–äº¤é›†
        preferred_final_mask = (preferred_answer_mask & preferred_padding_mask)
        rejected_final_mask = (rejected_answer_mask & rejected_padding_mask)

        ## ==================== æ‰¹æ¬¡æœ‰æ•ˆæ€§æ£€æŸ¥ ====================

        # æ£€æŸ¥åå¥½å¯¹æ•°æ®æ˜¯å¦éƒ½æœ‰æœ‰æ•ˆçš„å›ç­”éƒ¨åˆ†
        preferred_min_tokens = preferred_final_mask.sum(dim=-1).min().item()
        rejected_min_tokens = rejected_final_mask.sum(dim=-1).min().item()

        if preferred_min_tokens == 0 or rejected_min_tokens == 0:
            print(f'âš ï¸ è·³è¿‡ç¬¬{batch_idx + 1}æ‰¹æ¬¡ï¼šåå¥½å¯¹æ•°æ®å›ç­”éƒ¨åˆ†ä¸è¶³')
            skipped_batches_count += 1
            continue  # è·³è¿‡å½“å‰æ‰¹æ¬¡

        ## ==================== æ¨¡å‹å‰å‘ä¼ æ’­ ====================

        # è®­ç»ƒæ¨¡å‹å¯¹åå¥½æ•°æ®çš„å‰å‘ä¼ æ’­
        preferred_logits = model(preferred_model_inputs).logits
        torch.cuda.empty_cache()  # æ¸…ç†GPUæ˜¾å­˜
        torch.cuda.ipc_collect()

        # è®­ç»ƒæ¨¡å‹å¯¹æ‹’ç»æ•°æ®çš„å‰å‘ä¼ æ’­
        rejected_logits = model(rejected_model_inputs).logits
        torch.cuda.empty_cache()  # æ¸…ç†GPUæ˜¾å­˜
        torch.cuda.ipc_collect()

        # å‚è€ƒæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            reference_preferred_logits = ref_model(preferred_model_inputs).logits.detach()
            reference_rejected_logits = ref_model(rejected_model_inputs).logits.detach()

        ## ==================== DPOæŸå¤±è®¡ç®— ====================
        """
        DPO (Direct Preference Optimization) è®ºæ–‡: https://arxiv.org/pdf/2305.18290.pdf
        æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡åå¥½å¯¹æ¯”å­¦ä¹ ï¼Œæ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹
        """

        # è®¡ç®—å¹³å‡å¯¹æ•°æ¦‚ç‡ (average_log_prob = True)
        # å‚è€ƒ: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L924

        ### è®­ç»ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡
        ### æ­£åœ¨å¾®è°ƒçš„æ¨¡å‹ï¼Œæ¥æ”¶åˆ°æ­£ä¾‹çš„logitsï¼Œè®¡ç®—å¯¹æ•°æ¦‚ç‡
        preferred_log_prob = _compute_average_log_probability(
            preferred_logits,
            preferred_target_labels,
            preferred_final_mask
        )
        rejected_log_prob = _compute_average_log_probability(
            rejected_logits,
            rejected_target_labels,
            rejected_final_mask
        )

        ### å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡
        reference_preferred_log_prob = _compute_average_log_probability(
            reference_preferred_logits,
            preferred_target_labels,
            preferred_final_mask
        )
        reference_rejected_log_prob = _compute_average_log_probability(
            reference_rejected_logits,
            rejected_target_labels,
            rejected_final_mask
        )

        ## ==================== å¥–åŠ±å’Œè¾¹é™…è®¡ç®— ====================

        # è®¡ç®—éšå¼å¥–åŠ± (åŸºäºKLæ•£åº¦)
        preferred_implicit_reward = beta * (preferred_log_prob - reference_preferred_log_prob)
        rejected_implicit_reward = beta * (rejected_log_prob - reference_rejected_log_prob)

        # è®¡ç®—å¥–åŠ±è¾¹é™… (åå¥½æ•°æ®åº”è¯¥æœ‰æ›´é«˜çš„å¥–åŠ±)
        reward_margin = preferred_implicit_reward - rejected_implicit_reward

        # DPOæŸå¤±ï¼š-log(sigmoid(margin))
        preference_probability = torch.nn.functional.sigmoid(reward_margin)
        sample_losses = -torch.log(preference_probability)

        # æ‰¹æ¬¡å¹³å‡æŸå¤± + æ¢¯åº¦ç´¯ç§¯
        batch_average_loss = torch.nanmean(sample_losses) / gradient_accumulation_steps

        ## ==================== åå‘ä¼ æ’­å’Œä¼˜åŒ– ====================

        batch_average_loss.backward()

        # åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
        current_learning_rate = cosine_decay(
            batch_idx,
            warmup_steps,
            total_steps,
            max_lr,
            min_lr
        )

        for param_group in optimizer.param_groups:
            param_group['lr'] = current_learning_rate

        # æ¢¯åº¦ç´¯ç§¯å’Œæƒé‡æ›´æ–°
        is_accumulation_step = (batch_idx + 1) % gradient_accumulation_steps == 0
        is_final_batch = (batch_idx + 1) == total_batches

        if is_accumulation_step or is_final_batch:
            optimizer.step()  # æ›´æ–°æƒé‡
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦

        ## ==================== è®­ç»ƒæŒ‡æ ‡è®°å½• ====================

        # è®°å½•å„é¡¹è®­ç»ƒæŒ‡æ ‡ï¼ˆdetaché¿å…æ¢¯åº¦è¿½è¸ªï¼‰
        training_losses.append(batch_average_loss.detach().item() * gradient_accumulation_steps)
        preferred_log_probabilities.append(torch.nanmean(preferred_log_prob.detach()).item())
        rejected_log_probabilities.append(torch.nanmean(rejected_log_prob.detach()).item())
        preferred_rewards.append(torch.nanmean(preferred_implicit_reward.detach()).item())
        rejected_rewards.append(torch.nanmean(rejected_implicit_reward.detach()).item())
        reward_margins.append(torch.nanmean(reward_margin.detach()).item())

        ## ==================== è®­ç»ƒæ—¥å¿—è¾“å‡º ====================

        should_log = (batch_idx + 1) % log_iter == 0 or is_final_batch

        if should_log:
            # è®¡ç®—æœ€è¿‘æ‰¹æ¬¡çš„å¹³å‡æŒ‡æ ‡
            recent_loss = np.nanmean(training_losses[-log_iter:])
            recent_preferred_logprob = np.nanmean(preferred_log_probabilities[-log_iter:])
            recent_rejected_logprob = np.nanmean(rejected_log_probabilities[-log_iter:])
            recent_preferred_reward = np.nanmean(preferred_rewards[-log_iter:])
            recent_rejected_reward = np.nanmean(rejected_rewards[-log_iter:])
            recent_margin = np.nanmean(reward_margins[-log_iter:])

            # æ ¼å¼åŒ–è¾“å‡ºè®­ç»ƒçŠ¶æ€
            current_time = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f'â° æ—¶é—´: {current_time}')
            print(f'ğŸ“Š æ‰¹æ¬¡: {batch_idx + 1}/{total_batches}')
            print(f'ğŸ“ˆ æœ€è¿‘{log_iter}æ‰¹æ¬¡æŒ‡æ ‡:')
            print(f'   - å¹³å‡æŸå¤±: {recent_loss:.4f}')
            print(f'   - åå¥½å¯¹æ•°æ¦‚ç‡: {recent_preferred_logprob:.4f}')
            print(f'   - æ‹’ç»å¯¹æ•°æ¦‚ç‡: {recent_rejected_logprob:.4f}')
            print(f'   - åå¥½å¥–åŠ±: {recent_preferred_reward:.4f}')
            print(f'   - æ‹’ç»å¥–åŠ±: {recent_rejected_reward:.4f}')
            print(f'   - å¥–åŠ±è¾¹é™…: {recent_margin:.4f}')
            print(f'ğŸ¯ å­¦ä¹ ç‡: {current_learning_rate:.2e}')
            print('-' * 80)

            # è°ƒç”¨å¤–éƒ¨æ—¥å¿—è®°å½•
            log_call(batch_idx, recent_loss)

    ## ==================== è®­ç»ƒå®Œæˆæ€»ç»“ ====================

    print("ğŸ‰ DPOè®­ç»ƒå®Œæˆ!")
    print(f'ğŸ“Š è®­ç»ƒç»Ÿè®¡:')
    print(f'   - æ€»æ‰¹æ¬¡æ•°: {total_batches}')
    print(f'   - è·³è¿‡æ‰¹æ¬¡æ•°: {skipped_batches_count}')
    print(f'   - æœ‰æ•ˆæ‰¹æ¬¡æ•°: {total_batches - skipped_batches_count}')

    # è¾“å‡ºæœ€ç»ˆè®­ç»ƒæŒ‡æ ‡
    if training_losses:
        final_metrics = {
            'loss': np.nanmean(training_losses[-100:]),
            'preferred_logprob': np.nanmean(preferred_log_probabilities[-100:]),
            'rejected_logprob': np.nanmean(rejected_log_probabilities[-100:]),
            'preferred_reward': np.nanmean(preferred_rewards[-100:]),
            'rejected_reward': np.nanmean(rejected_rewards[-100:]),
            'margin': np.nanmean(reward_margins[-100:])
        }

        print(f'ğŸ¯ æœ€ç»ˆæŒ‡æ ‡ (æœ€è¿‘100æ‰¹æ¬¡å¹³å‡):')
        for metric_name, metric_value in final_metrics.items():
            print(f'   - {metric_name}: {metric_value:.4f}')

    if skipped_batches_count > 0:
        skip_ratio = skipped_batches_count / total_batches * 100
        print(f'âš ï¸ è·³è¿‡æ‰¹æ¬¡å æ¯”: {skip_ratio:.2f}%')
        if skip_ratio > 10:
            print('ğŸ’¡ å»ºè®®: è·³è¿‡æ‰¹æ¬¡è¿‡å¤šï¼Œè€ƒè™‘å¢åŠ æœ€å¤§åºåˆ—é•¿åº¦æˆ–ä¼˜åŒ–æ•°æ®é¢„å¤„ç†')

    model.save_pretrained(str(config.QWEN2_5_0_5B_DPO_PATH))
    tokenizer.save_pretrained(str(config.QWEN2_5_0_5B_DPO_PATH))
