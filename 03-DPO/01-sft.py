import config
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from dataclasses import dataclass
import numpy as np
import datasets
import time

log_path = str(config.QWEN2_5_0_5B_SFT_LOG_PATH)


@dataclass
class SFTConfig:
    max_length: int = 2500
    batch_size: int = 2
    gradient_accumulation_steps: int = 8
    log_iter: int = 400
    max_lr: float = 2e-5
    min_lr: float = 2e-6
    warmup_steps: int = 1000


def tokenize_and_format(data):
    input_ids = tokenizer.apply_chat_template(
        data,
        tokenize=True,
        add_generation_prompt=False,
        truncation=True,
        max_length=2500,
    )
    return input_ids


# å®šä¹‰ä¸€ä¸ªæ—¥å¿—è®°å½•å‡½æ•°
def log_call(iters, iters_average_loss):
    with open(log_path, "a") as my_file:
        my_file.write(
            f'time:{time.strftime("%Y-%m-%d, %H:%M:%S")},  iters:{iters + 1}, iters_average_Loss:{iters_average_loss:.4f}\n')


def linear_warmup(current_step, warmup_steps, max_lr):
    if current_step < warmup_steps:
        return max_lr * current_step / warmup_steps
    else:
        return max_lr


def cosine_decay(current_step, warmup_steps, total_steps, max_lr, min_lr):
    if current_step < warmup_steps:
        return linear_warmup(current_step, warmup_steps, max_lr)
    else:
        progress = (current_step - warmup_steps) / (total_steps - warmup_steps)
        decay = 0.5 * (1 + np.cos(np.pi * progress))
        return (max_lr - min_lr) * decay + min_lr


def create_answer_mask(input_ids, tokenizer):
    """
    åˆ›å»ºä»…å¯¹åŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¡ç®—æŸå¤±çš„æ©ç 

    Args:
        input_ids: è¾“å…¥tokenåºåˆ— [batch_size, seq_len]
        tokenizer: åˆ†è¯å™¨

    Returns:
        answer_mask: åŠ©æ‰‹å›ç­”éƒ¨åˆ†ä¸º1ï¼Œå…¶ä»–éƒ¨åˆ†ä¸º0çš„æ©ç 
    """
    batch_size, seq_len = input_ids.shape
    answer_mask = torch.zeros_like(input_ids)

    # è·å–ç»“æŸæ ‡è®°çš„token id
    eos_token_id = tokenizer.encode('<|im_end|>')[0]

    for batch_idx in range(batch_size):
        # æ‰¾åˆ°æ‰€æœ‰ <|im_end|> çš„ä½ç½®
        eos_positions = torch.where(
            input_ids[batch_idx] == eos_token_id
        )[0].tolist()

        if len(eos_positions) < 2:  # è‡³å°‘éœ€è¦userå’Œassistantå„ä¸€ä¸ªç»“æŸæ ‡è®°
            continue

        # è§£æå¯¹è¯è½®æ¬¡
        user_ends, assistant_ends = \
            _parse_conversation_turns(eos_positions)

        # ä¸ºæ¯ä¸ªåŠ©æ‰‹å›ç­”è®¾ç½®æ©ç 
        _set_answer_masks(
            answer_mask[batch_idx],
            user_ends,
            assistant_ends,
            seq_len
        )

    return answer_mask


def _parse_conversation_turns(eos_positions):
    """
    è§£æå¯¹è¯è½®æ¬¡ï¼Œåˆ†ç¦»ç”¨æˆ·å’ŒåŠ©æ‰‹çš„ç»“æŸä½ç½®

    å¯¹è¯æ ¼å¼ï¼š
    <|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n{assistant_msg}<|im_end|>\n

    eos_positions[0]: systemç»“æŸ (å¦‚æœæœ‰)
    eos_positions[1]: ç¬¬1è½®userç»“æŸ
    eos_positions[2]: ç¬¬1è½®assistantç»“æŸ
    eos_positions[3]: ç¬¬2è½®userç»“æŸ
    eos_positions[4]: ç¬¬2è½®assistantç»“æŸ
    ...
    """
    # è·³è¿‡systemç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼Œä»ç¬¬ä¸€ä¸ªuserå¼€å§‹
    conversation_eos = eos_positions[1:]  # å»æ‰systemçš„<im_end>

    # å¶æ•°ç´¢å¼•ï¼šuserç»“æŸä½ç½®ï¼Œå¥‡æ•°ç´¢å¼•ï¼šassistantç»“æŸä½ç½®
    user_ends = [pos + 1 for pos in conversation_eos[::2]]  # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»0å¼€å§‹
    assistant_ends = [pos + 1 for pos in conversation_eos[1::2]]  # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»1å¼€å§‹

    return user_ends, assistant_ends


def _set_answer_masks(mask, user_ends, assistant_ends, seq_len):
    """
    ä¸ºåŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¾ç½®æ©ç 

    Args:
        mask: å½“å‰æ ·æœ¬çš„æ©ç  [seq_len]
        user_ends: ç”¨æˆ·æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨
        assistant_ends: åŠ©æ‰‹æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨
        seq_len: åºåˆ—é•¿åº¦
    """
    num_user_turns = len(user_ends)
    num_assistant_turns = len(assistant_ends)

    if num_user_turns == num_assistant_turns:
        # å®Œæ•´å¯¹è¯ï¼šæ¯è½®éƒ½æœ‰ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”
        for user_end, assistant_end in zip(user_ends, assistant_ends):
            answer_start = user_end + 3  # è·³è¿‡ <|im_start|>assistant\n
            answer_end = assistant_end - 1  # ä¸åŒ…å« <|im_end|>
            mask[answer_start:answer_end] = 1

    elif num_user_turns == num_assistant_turns + 1:
        # æœªå®Œæˆå¯¹è¯ï¼šæœ€åä¸€è½®åŠ©æ‰‹å›ç­”è¢«æˆªæ–­

        # å¤„ç†å®Œæ•´çš„å¯¹è¯è½®æ¬¡
        for user_end, assistant_end in zip(user_ends[:-1], assistant_ends):
            answer_start = user_end + 3
            answer_end = assistant_end - 1
            mask[answer_start:answer_end] = 1

        # å¤„ç†æœ€åä¸€è½®è¢«æˆªæ–­çš„åŠ©æ‰‹å›ç­”
        last_user_end = user_ends[-1]
        last_answer_start = last_user_end + 3
        mask[last_answer_start:] = 1  # åˆ°åºåˆ—ç»“å°¾


if __name__ == '__main__':
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model_path = str(config.QWEN2_5_0_5B_PATH)

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype="auto",
        device_map="auto"
    )
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    print(model.generation_config)

    model.generation_config.do_sample = True
    model.generation_config.eos_token_id = [151645, 151643]
    model.generation_config.pad_token_id = 151643
    model.generation_config.temperature = 0.7
    model.generation_config.top_p = 0.8
    model.generation_config.top_k = 20
    model.generation_config.repetition_penalty = 1.05

    print(model.generation_config)

    ultrachat_200k_data = datasets.load_dataset(str(config.ULTRA_CHAT_200K_PATH))
    print(ultrachat_200k_data["train_sft"][0])

    ## ç”Ÿæˆè®­ç»ƒæ•°æ®çš„token id
    chosen_input_ids_list = []
    i = 0
    while True:
        data = ultrachat_200k_data['train_sft'][i]['messages']
        # æ·»åŠ  **ç³»ç»Ÿæç¤ºè¯**
        data.insert(
            0,
            {"content": "You are a helpful assistant", "role": "system"}
        )
        input_ids = tokenize_and_format(data)
        chosen_input_ids_list.append(input_ids)
        i += 1
        if i % 1000 == 0:
            print(f"å·²å¤„ç†{i}æ¡æ•°æ®")
        if i == 50000:  # len(ultrachat_200k_data['train_sft']):
            break
    print('-' * 70)

    batch_size = SFTConfig.batch_size
    gradient_accumulation_steps = SFTConfig.gradient_accumulation_steps
    log_iter = SFTConfig.log_iter
    max_lr = SFTConfig.max_lr
    min_lr = SFTConfig.min_lr
    warmup_steps = SFTConfig.warmup_steps
    total_steps = len(chosen_input_ids_list) // batch_size
    optimizer = torch.optim.AdamW(filter(
        lambda p: p.requires_grad,
        model.parameters()
    ), lr=max_lr)
    trainable_parameters_num = sum(p.numel() for p in filter(
        lambda p: p.requires_grad,
        model.parameters()))
    print(f"è®­ç»ƒå‚æ•°æ•°é‡ï¼š{trainable_parameters_num}")

    with open(log_path, "a") as my_file:
        my_file.write(
            f'time:{time.strftime("%Y-%m-%d, %H:%M:%S")},batch_size:{batch_size},trainable_parameters_num:{trainable_parameters_num}, warmup_steps:{warmup_steps},max_lr:{max_lr}, min_lr:{min_lr}\n')

    model = model.to(device)
    model.train()
    training_losses = []
    model.zero_grad()  # è®­ç»ƒå¼€å§‹æ—¶æ¸…ç©ºæ¢¯åº¦
    skipped_batches_count = 0

    total_batches = len(chosen_input_ids_list) // batch_size

    for batch_idx in range(total_batches):
        ## ==================== æ•°æ®å‡†å¤‡é˜¶æ®µ ====================

        # è·å–å½“å‰æ‰¹æ¬¡çš„åŸå§‹æ•°æ®
        current_batch_sequences = chosen_input_ids_list[
            batch_idx * batch_size: (batch_idx + 1) * batch_size
        ]

        # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œç”¨äºpaddingå¯¹é½
        max_sequence_length = max([len(sequence) for sequence in current_batch_sequences])

        ### å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œå³å¡«å……ï¼Œä½¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´ä»¥ä¾¿å¹¶è¡Œè®¡ç®—
        padded_sequences_list = []
        pad_token_id = model.generation_config.eos_token_id[-1]

        for seq_idx in range(batch_size):
            # åŸå§‹çš„ä¸€æ¡è®­ç»ƒæ•°æ®
            original_sequence = current_batch_sequences[seq_idx]
            # è¦å¡«å……çš„é•¿åº¦
            padding_length = max_sequence_length - len(original_sequence)

            # ä½¿ç”¨EOS tokenè¿›è¡Œå³å¡«å……
            padded_sequence = torch.nn.functional.pad(
                torch.tensor(original_sequence),
                (0, padding_length),
                mode='constant',
                value=pad_token_id
            ).tolist()

            padded_sequences_list.append(padded_sequence)

        # è½¬æ¢ä¸ºå¼ é‡
        batch_input_tensor = torch.tensor(padded_sequences_list)

        ## ==================== æ„å»ºè¾“å…¥è¾“å‡ºå¯¹ ====================

        # æ„å»ºå› æœè¯­è¨€æ¨¡å‹çš„è¾“å…¥è¾“å‡ºå¯¹ï¼šx->yï¼ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼‰
        model_inputs = batch_input_tensor[:, :-1].to(device)  # è¾“å…¥ï¼šå‰n-1ä¸ªtoken
        target_labels = batch_input_tensor[:, 1:].to(device)  # æ ‡ç­¾ï¼šån-1ä¸ªtoken

        ## ==================== æ„å»ºè®­ç»ƒæ©ç  ====================

        # æ„å»ºæ©ç çŸ©é˜µæ¥æ§åˆ¶æŸå¤±è®¡ç®—èŒƒå›´
        # 1. padding_maskï¼šæ ‡è¯†å“ªäº›ä½ç½®æ˜¯å¡«å……tokenï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰
        # 2. answer_maskï¼šæ ‡è¯†å“ªäº›ä½ç½®æ˜¯åŠ©æ‰‹å›ç­”éƒ¨åˆ†ï¼ˆåªå¯¹å›ç­”è®¡ç®—æŸå¤±ï¼‰

        ### ã€å¡«å……æ©ç ã€‘ï¼šéå¡«å……tokenä¸º1ï¼Œå¡«å……tokenä¸º0
        ### padding_maskä¸­çš„é—®é¢˜éƒ¨åˆ†çš„æ©ç ä¹Ÿæ˜¯1
        padding_mask = torch.where(target_labels == pad_token_id, 0, 1)

        ### ã€å›ç­”æ©ç ã€‘ï¼šåªæœ‰åŠ©æ‰‹å›ç­”éƒ¨åˆ†ä¸º1ï¼Œå…¶ä»–éƒ¨åˆ†ä¸º0
        assistant_answer_mask = create_answer_mask(model_inputs, tokenizer)

        ### ã€ç»„åˆæ©ç ã€‘ï¼šåŒæ—¶æ»¡è¶³"éå¡«å……"ä¸”"æ˜¯å›ç­”éƒ¨åˆ†"çš„tokenæ‰è®¡ç®—æŸå¤±
        ### å–å‡ºäº¤é›†ï¼Œå°±æ˜¯çœŸæ­£è¦è®¡ç®—çš„å›ç­”éƒ¨åˆ†
        final_loss_mask = (assistant_answer_mask & padding_mask)

        ## ==================== æ‰¹æ¬¡æœ‰æ•ˆæ€§æ£€æŸ¥ ====================

        # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦æœ‰æ•ˆï¼šå¦‚æœæŸä¸ªæ ·æœ¬çš„å›ç­”éƒ¨åˆ†å®Œå…¨ä¸ºç©ºï¼Œåˆ™è·³è¿‡è¯¥æ‰¹æ¬¡
        # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨é—®é¢˜è¿‡é•¿å¯¼è‡´å›ç­”éƒ¨åˆ†è¢«æˆªæ–­æ—¶
        tokens_per_sample = final_loss_mask.sum(dim=-1)  # æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆå›ç­”tokenæ•°
        min_answer_tokens = tokens_per_sample.min().item()  # æœ€å°‘çš„æœ‰æ•ˆtokenæ•°

        if min_answer_tokens == 0:
            print(f'âš ï¸ è·³è¿‡ç¬¬{batch_idx + 1}æ‰¹æ¬¡ï¼šå›ç­”éƒ¨åˆ†æ•°æ®ä¸è¶³')
            skipped_batches_count += 1
            continue  # è·³è¿‡å½“å‰æ‰¹æ¬¡

        ## ==================== æ¨¡å‹å‰å‘ä¼ æ’­ ====================

        # æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹é¢„æµ‹çš„logits
        # [batch_size, seq_length, vocab_size]
        model_logits = model(model_inputs).logits

        ## ==================== æŸå¤±è®¡ç®— ====================

        # è®¡ç®—å¸¦æ©ç çš„äº¤å‰ç†µæŸå¤±
        # æ­¥éª¤ï¼šlogits -> softmax -> log -> gather -> è´Ÿå¯¹æ•°ä¼¼ç„¶ -> æ©ç è¿‡æ»¤ -> å¹³å‡

        # 1. è®¡ç®—æ¯ä¸ªtokençš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œ
        # å½¢çŠ¶ï¼š[batch_size, seq_len, vocab_size]
        log_probabilities = torch.log(torch.softmax(model_logits, dim=-1))
        # ä½¿ç”¨çœŸæ­£çš„ç›®æ ‡tokenå–å‡ºvocab_sizeé•¿åº¦çš„æ•°ç»„ä¸­tokenå¯¹åº”çš„å¯¹æ•°æ¦‚ç‡
        # å½¢çŠ¶ï¼š[batch_size, seq_len]
        gathered_log_probs = torch.gather(
            log_probabilities,
            dim=-1,
            index=target_labels.unsqueeze(2)
        )
        negative_log_likelihood = gathered_log_probs * (-1)  # è´Ÿå¯¹æ•°ä¼¼ç„¶
        token_losses = negative_log_likelihood.squeeze(2)

        # 2. åº”ç”¨æ©ç å¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æŸå¤±
        masked_token_losses = torch.mul(token_losses, final_loss_mask)
        sample_losses = masked_token_losses.sum(dim=-1) / final_loss_mask.sum(dim=-1)

        # 3. è®¡ç®—æ‰¹æ¬¡å¹³å‡æŸå¤±å¹¶åº”ç”¨æ¢¯åº¦ç´¯ç§¯
        batch_average_loss = torch.nanmean(sample_losses) / gradient_accumulation_steps

        ## ==================== åå‘ä¼ æ’­å’Œä¼˜åŒ– ====================

        # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        batch_average_loss.backward()

        # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆä½™å¼¦è¡°å‡ + é¢„çƒ­ï¼‰
        current_learning_rate = cosine_decay(
            batch_idx,
            warmup_steps,
            total_steps,
            max_lr,
            min_lr
        )

        # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] = current_learning_rate

        # æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æˆ–æœ€åä¸€ä¸ªæ‰¹æ¬¡æ—¶æ›´æ–°æƒé‡
        is_accumulation_step = (batch_idx + 1) % gradient_accumulation_steps == 0
        is_final_batch = (batch_idx + 1) == total_batches

        if is_accumulation_step or is_final_batch:
            optimizer.step()  # æ›´æ–°æ¨¡å‹æƒé‡
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦ç¼“å­˜

        ## ==================== è®­ç»ƒæ—¥å¿—è®°å½• ====================

        # è®°å½•å½“å‰æ‰¹æ¬¡çš„æŸå¤±ï¼ˆè¿˜åŸæ¢¯åº¦ç´¯ç§¯çš„ç¼©æ”¾ï¼‰
        actual_batch_loss = batch_average_loss.item() * gradient_accumulation_steps
        training_losses.append(actual_batch_loss)

        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        should_log = (batch_idx + 1) % log_iter == 0 or is_final_batch

        if should_log:
            # è®¡ç®—æœ€è¿‘å‡ ä¸ªæ‰¹æ¬¡çš„å¹³å‡æŸå¤±
            recent_losses = training_losses[-log_iter:]
            recent_average_loss = np.nanmean(recent_losses)

            # è¾“å‡ºè®­ç»ƒçŠ¶æ€
            current_time = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f'â° æ—¶é—´: {current_time} | '
                  f'ğŸ“Š æ‰¹æ¬¡: {batch_idx + 1}/{total_batches} | '
                  f'ğŸ“ˆ æœ€è¿‘{len(recent_losses)}æ‰¹æ¬¡å¹³å‡æŸå¤±: {recent_average_loss:.4f} | '
                  f'ğŸ¯ å­¦ä¹ ç‡: {current_learning_rate:.2e}')

            # è°ƒç”¨å¤–éƒ¨æ—¥å¿—è®°å½•å‡½æ•°
            log_call(batch_idx, recent_average_loss)

    ## ==================== è®­ç»ƒå®Œæˆæ€»ç»“ ====================

    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f'ğŸ“Š è®­ç»ƒç»Ÿè®¡:')
    print(f'   - æ€»æ‰¹æ¬¡æ•°: {total_batches}')
    print(f'   - è·³è¿‡æ‰¹æ¬¡æ•°: {skipped_batches_count}')
    print(f'   - æœ‰æ•ˆæ‰¹æ¬¡æ•°: {total_batches - skipped_batches_count}')
    print(f'   - æœ€ç»ˆå¹³å‡æŸå¤±: {np.nanmean(training_losses[-100:]):.4f}')

    if skipped_batches_count > 0:
        skip_ratio = skipped_batches_count / total_batches * 100
        print(f'âš ï¸ è·³è¿‡æ‰¹æ¬¡å æ¯”: {skip_ratio:.2f}%')
        if skip_ratio > 10:
            print('ğŸ’¡ å»ºè®®: è·³è¿‡æ‰¹æ¬¡è¿‡å¤šï¼Œè€ƒè™‘å¢åŠ æœ€å¤§åºåˆ—é•¿åº¦æˆ–ä¼˜åŒ–æ•°æ®é¢„å¤„ç†')

    model_sft_path = str(config.QWEN2_5_0_5B_SFT_PATH)
    model.save_pretrained(model_sft_path)
    tokenizer.save_pretrained(model_sft_path)
